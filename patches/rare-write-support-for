Bottom: aa8c215b362d358dd5df1ec02ee1602a96dc1d41
Top:    9b1852485096230ba69d3c4be7f92d9ec6769df8
Author: Igor Stoppa <igor.stoppa@huawei.com>
Date:   2018-05-20 15:05:59 +0300

Rare Write support for statically allocated memory

The options currently available for protecting memory are limited to:
* const: memory is write protected from early boot stages.
* __ro_after_init: memory is writable during the init phase.

Which means that if some data is modified very seldomly (or even just
once), but after init, it is not possible to give it any additional level
of protection.

Reasons for wanting additional protection on the data:
* accidental overwrites due to bugs / malfunctions.
* malicious attacks from userspace, exploiting some vulnerability.

The former type of protection is beneficial for any data.
The latter type is more relevant for data that might be targeted by an
attack to the system integrity.

The most common type of attack aims at flipping some kernel state variable,
often statically allocated. The purpose of the attack is to alter the
execution flow, for example to bypass existing safety measures.

These types of attack rely on having read/write capability to kernel
memory (write is typically limited to unprotected kernel data).
Such attacks will usually rely on identifying a vulnerability in some
kernel API, then exploit it, to read/alter arbitrary locations.

This patch is intended to prevent such direct attacks.

However, it doesn't prevent more sophisticated attacks, based, for
example, on walking "manually" the page table and altering attributes of
victim pages.

(Note: what was just described is an intrinsic vulnerability that
affects also __ro_after_init.
const data is less affected because the compiler can perform additional
optimizations and replace references to the constant with an immediate,
where possible.)

Nor, does this patch prevent Control Flow based attacks.
For those, a specific compiler plugin is needed.

However, even with these limitations, the patch still provides higher
protection than the current state of affairs, where such variables are
directly writable, all the time.
And the attacker is forced to execute far more difficult tasks, which
are more likely to fail.

But the patch must not introduce new weaknesses, nor compromise overall
performance.

The mechanism used for performing the alteration (rare write) is to:
* identify the page - or pages - targeted
* sequentially, remap them to a random location, with write permission
* alter the interested locations
* destroy the writable mapping

To avoid introducing weaknesses:
* the rewritable memory is contained in a subsection with boundaries
* prior to any change, it is confirmed to belong to the rewritable section
* the altering functionality is inlined, to minimize code reuse attacks

The reason for performing the remapping is that static variables are the
easiest to locate in memory and therefore, unprotecting the active
mapping would be risky, at best.
An attacker might try to busy loop on one core/thread, while a rare
write is ongoing, to alter anything that is contained in the same page of
the data being written, if rare write was implemented by unprotecting
the linear mapping.
However, by remapping the page to a separate address, the protections on
its initial mapping is preserved and the attacker has the much harder task
to figure out where the page is being remapped, before being able to
take advantage of it.

Thus, the patch does not affect overall performance, since it doesn't
introduce any locking, to keep other cores bay.

Furthermore, from the perspective of ensuring integrity, in a scenario with
multiple writers, as long as they do not try write to the same memory
locations, there is no need to introduce serialization.

If two or more cores/threads were to map the same page to different
addresses, writing to different locations within the same page, the cache
coherency mechanism, which is performed at physical level, would ensure to
propagate the changes.
Locking is needed only in the typical case where multiple writers want
to alter the very same memory, but that is not different from any normal
memory assignment.

The individual execution of a rare-write operation has, of course,
longer duration than a simple assignement to unprotected memory, however
it doesn't require stalling any other core/thread, therefore leaving any
penalty only on the core/thread performing the rare-write.

The special modifier added is similar in both name and behavior, to
__ro_after_init: __rare_write_after_init.

For the sake of having one single implementation of the functionality,
working with both linear and virtually linear memory, the core
rare_write function supports selection of memory type, which gets
optimized away at compile time once a specific type is chosen.
This is needed to invoke the correct function for obtaining the page
structure, without adding extra runtime tests. Furthermore it reduces
the chances that an attacker would use it with the unintended type of
addresses.

Signed-off-by: Igor Stoppa <igor.stoppa@huawei.com>
CC: Carlos Chinea Perez <carlos.chinea.perez@huawei.com>
CC: Remi Denis Courmont <remi.denis.courmont@huawei.com>
CC: Ashish Raj <araj@elisanet.fi>


---

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index af240573e482..d9216c969308 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -294,6 +294,17 @@
 	. = VMLINUX_SYMBOL(__start_init_task) + THREAD_SIZE;		\
 	VMLINUX_SYMBOL(__end_init_task) = .;
 
+/*
+ * Allow architectures to handle rare_write_after_init data on their
+ * own by defining an empty RARE_WRITE_AFTER_INIT_DATA.
+ */
+#ifndef RARE_WRITE_AFTER_INIT_DATA
+#define RARE_WRITE_AFTER_INIT_DATA					\
+	VMLINUX_SYMBOL(__start_rare_write_after_init) = .;		\
+	*(.data..rare_write_after_init)					\
+	VMLINUX_SYMBOL(__end_rare_write_after_init) = .;
+#endif
+
 /*
  * Allow architectures to handle ro_after_init data on their
  * own by defining an empty RO_AFTER_INIT_DATA.
@@ -314,6 +325,7 @@
 		VMLINUX_SYMBOL(__start_rodata) = .;			\
 		*(.rodata) *(.rodata.*)					\
 		RO_AFTER_INIT_DATA	/* Read only after init */	\
+		RARE_WRITE_AFTER_INIT_DATA /* Rare write after init */	\
 		KEEP(*(__vermagic))	/* Kernel version magic */	\
 		. = ALIGN(8);						\
 		VMLINUX_SYMBOL(__start___tracepoints_ptrs) = .;		\
diff --git a/include/linux/cache.h b/include/linux/cache.h
index 750621e41d1c..af118f90551a 100644
--- a/include/linux/cache.h
+++ b/include/linux/cache.h
@@ -31,6 +31,23 @@
 #define __ro_after_init __attribute__((__section__(".data..ro_after_init")))
 #endif
 
+/*
+ * __rare_write_after_init is used to mark objects that after init cannot
+ * be modified directly (i.e. after mark_rodata_ro() has been called).
+ * These objects become effectively read-only, from the perspective of
+ * performing a direct write, like a variable assignment.
+ * However, they can be altered through a dedicated function.
+ * It is intended for those objects which are occasionally modified after
+ * init, however are they modified so seldomly that the extra cost from
+ * the indirect modification is either negligible or worth paying, for the
+ * sake of the hardening gained.
+ */
+#ifndef __rare_write_after_init
+#define __rare_write_after_init \
+		__attribute__((__section__(".data..rare_write_after_init")))
+#endif
+
+
 #ifndef ____cacheline_aligned
 #define ____cacheline_aligned __attribute__((__aligned__(SMP_CACHE_BYTES)))
 #endif
diff --git a/include/linux/rare_write.h b/include/linux/rare_write.h
new file mode 100644
index 000000000000..b4c489e2afd3
--- /dev/null
+++ b/include/linux/rare_write.h
@@ -0,0 +1,169 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * rare_write.h: Header for rare writes to statically allocated variables
+ *
+ * (C) Copyright 2018 Huawei Technologies Co. Ltd.
+ * Author: Igor Stoppa <igor.stoppa@huawei.com>
+ *
+ * Support for modifying "variables" residing in protected memory.
+ * They are write-protected against direct writes and can be altered only
+ * through special means.
+ */
+
+#ifndef _LINUX_RARE_WRITE_H
+#define _LINUX_RARE_WRITE_H
+
+#include <linux/mm.h>
+
+extern long __start_rare_write_after_init;
+extern long __end_rare_write_after_init;
+
+enum rare_write_type {
+	RARE_WRITE_VIRT_ADDR,
+	RARE_WRITE_VMALLOC_ADDR,
+};
+
+static __always_inline
+bool rare_write_check_boundaries(const void *dst, size_t size)
+{
+	size_t start = (size_t)&__start_rare_write_after_init;
+	size_t end = (size_t)&__end_rare_write_after_init;
+	size_t low = (size_t)dst;
+	size_t high = (size_t)dst + size;
+
+	return likely(start <= low && low < high && high <= end);
+}
+
+/*
+ * This is the core of the rare write functionality.
+ * It doesn't perform any check on the validity of the target.
+ * The wrapper using it is supposed to apply sensible verification
+ * criteria, depending on the specific use-case and, to avoid unnecessary
+ * run-time checks, also specify the type of memory being modified.
+ */
+static __always_inline
+bool __raw_rare_write(const void *dst, const void *src,
+		      enum rare_write_type type, size_t n_bytes)
+{
+	size_t size;
+
+	while (n_bytes) {
+		struct page *page;
+		void *base;
+		unsigned long offset;
+		size_t offset_complement;
+
+		if (type == RARE_WRITE_VIRT_ADDR)
+			page = virt_to_page(dst);
+		else
+			page = vmalloc_to_page(dst);
+		base = vmap(&page, 1, VM_MAP, PAGE_KERNEL);
+		if (WARN(!base, "failed to remap rare-write page"))
+			return false;
+		offset = (unsigned long)dst & ~PAGE_MASK;
+		offset_complement = ((size_t)PAGE_SIZE) - offset;
+		size = min(((int)n_bytes), ((int)offset_complement));
+		memcpy(base + offset, src, size);
+		vunmap(base);
+		dst += size;
+		src += size;
+		n_bytes -= size;
+	}
+	return true;
+}
+
+static __always_inline
+bool __rare_write(const void *dst, const void *src, size_t n_bytes)
+{
+
+	if (WARN(!rare_write_check_boundaries(dst, n_bytes),
+		 "Not a valid rare_write destination."))
+		return false;
+	return __raw_rare_write(dst, src, RARE_WRITE_VIRT_ADDR, n_bytes);
+
+}
+
+static __always_inline
+bool rare_write_char(const char *dst, const char val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_short(const short *dst, const short val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_ushort(const unsigned short *dst, const unsigned short val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_int(const int *dst, const int val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_uint(const unsigned int *dst, const unsigned int val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_long(const long *dst, const long val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_ulong(const unsigned long *dst, const unsigned long val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_longlong(const long long *dst, const long long val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_ulonglong(const unsigned long long *dst,
+			  const unsigned long long val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_ptr(const void *dst, const void *val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+#define __rare_write_simple(dst_ptr, src_ptr)				\
+	__rare_write(dst_ptr, src_ptr, sizeof(*(src_ptr)))
+
+#define __rare_write_safe(dst_ptr, src_ptr,				\
+			  unique_dst_ptr, unique_src_ptr)		\
+({									\
+	typeof(dst_ptr) unique_dst_ptr = (dst_ptr);			\
+	typeof(src_ptr) unique_src_ptr = (src_ptr);			\
+									\
+	__rare_write(unique_dst_ptr, unique_src_ptr,			\
+		     sizeof(*(unique_src_ptr)));			\
+})
+
+#define rare_write(dst_ptr, src_ptr)					\
+	__builtin_choose_expr(__typecheck(dst_ptr, src_ptr),		\
+			      __rare_write_simple(dst_ptr, src_ptr),	\
+			      __rare_write_safe(dst_ptr, src_ptr,	\
+						__UNIQUE_ID(__dst_ptr),	\
+						__UNIQUE_ID(__src_ptr)))
+
+#define rare_write_array(dst_ptr, src_ptr, size)			\
+	__rare_write(dst_ptr, src_ptr, size)
+#endif
