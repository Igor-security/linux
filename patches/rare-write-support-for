Bottom: f7d66ebf624aa8f7da8ab9c2fcb2c783017cec8b
Top:    0bf9f2192443f6ac1b5d37678277cbd0d34f97ec
Author: Igor Stoppa <igor.stoppa@huawei.com>
Date:   2018-05-20 15:05:59 +0300

Rare Write support for statically allocated memory

The options currently available for protecting statically allocated memory
are limited to:
* const: memory is write protected from early boot stages.
* __ro_after_init: memory is writable during the init phase.

Data which is modified very seldomly (or even just once), but after init,
cannot benefit from any additional level of protection.

Reasons for wanting additional protection on the data:
* accidental overwrites due to bugs / malfunctions.
* malicious attacks, typically from userspace, exploiting some vulnerability.

The former type of protection is beneficial for any data.
The latter type is more relevant for data that might be targeted by an
attack to the system integrity.

The most common type of attack aims at flipping some kernel state variable,
often statically allocated. The purpose of the attack is to alter the
execution flow, for example to bypass existing safety measures.

These types of attack rely on having read/write capability to kernel
memory (write is typically limited to unprotected kernel data).
Such attacks will usually exploit an unpatched vulnerability in some
kernel API, then exploit it, to read/alter arbitrary locations.

This patch is intended to make such direct attacks harder to perform.

Harder but not impossible, because if the attacker can read/write kernel
memory, there is still the possibility to undo whatever protection the
kernel might have setup, for example using jop/rop or a
page-reprogramming algorythm from user space.

This sort of intrinsic vulnerability affects also __ro_after_init,
though.

const data is less affected in those cases where the compiler can perform
additional optimizations and replace references to the constant with an
immediate value. In every other case, it's equally affected.

Nor, does this patch prevent Control Flow based attacks.
For those, other means are required, such as specific compiler plugins and
additional support from the hardware, like the features surfacing on the
most recent ARM and Intel architectures.

However, even with these limitations, the patch still provides higher
protection than the current state of affairs, where such variables are
directly writable, all the time.
And the attacker is forced to execute far more difficult tasks, which
require specific actions that could be detected more easily, like a page
walk performed from userspace.

But the patch must not introduce new weaknesses, nor compromise overall
performance.

The mechanism used for performing the alteration (rare write) is to:
* identify the page - or pages - targeted
* one by one, remap them to a random location, with write permission
* alter the interested locations
* destroy the writable mapping

To avoid introducing weaknesses:
* the rewritable memory is contained in a subsection with boundaries
* prior to any change, it is confirmed to belong to the rewritable section
* the altering functionality is inlined, to minimize code reuse attacks
  by allowing the compiler to generate less-generic code.

The reason for performing the remapping is that static variables are the
easiest to locate in memory and, therefore, unprotecting the active
mapping would be risky, at best.
By directly unprotecting the primary mapping, an attacker might try to busy
loop on another core/thread, while a rare write is ongoing, to take
advantage of the time window when the data is writable, at its primary
address. Furthermore, the attacker could alter anything that is contained
in the same page of the data being written to.
However, by remapping the page to a separate address, the protections on
its initial mapping is preserved and the attacker has the much harder task
to figure out where the page is being remapped, before being able to
take advantage of it.

To prevent an unlikely - but possible - attack from irq context, local
irqs are disabled for the duration of a remapping.

Other types of attack, which might try to reduce the randomness of the
remapping are still possible and should be addressed separately as any
attack that tries to reduce the entropy of the source of randomness.

The patch does not affect overall performance, since it doesn't introduce
any global locking, to keep other cores at bay.
It has limited impact on responsiveness to interrupts, which can be
quantified as the time taken for establishing the new mapping, copying
the data (max 1 page) and tearing down the temporary mapping.

From the perspective of ensuring integrity, in a scenario with
multiple writers, as long as they do not try write to the same memory
locations, there is no need to introduce serialization.

If two or more cores/threads were to map the same page to different
addresses, writing to different locations within the same page, the cache
coherency mechanism, which is performed at physical level, would ensure to
propagate the changes.
Locking is needed only in the typical case where multiple writers want
to alter the very same memory, but that is not different from any normal
memory assignment.

The individual execution of a rare-write operation has, of course,
longer duration than a simple assignement to unprotected memory, however
it doesn't require stalling any other core/thread, therefore leaving any
penalty only on the core/thread performing the rare-write.

Writes are performed through __write_once_size(), to ensure that atomicy
is preserved, where it might matter (ex: update of a pointer).

The special modifier added is similar in both name and behavior, to
__ro_after_init: __rare_write_after_init.

However, there is one major difference: in this case the interval that
is reserved for static rare writes MUST be page aligned, so that other
types of data do not share any page, to avoid mixing data types with
different access patterns, which would either leave unprotected some
data that is suppoed to be read-only, or it would write protect some
other data which is supposed to be always writable.

For the sake of having one single implementation of the functionality,
working with both linear and virtually linear memory, the core
rare_write function supports selection of memory type, which gets
optimized away at compile time once a specific type is chosen.
This is needed to invoke the correct function for obtaining the page
structure, without adding extra runtime tests. Furthermore it reduces
the chances that an attacker would use it with the unintended type of
addresses.

Signed-off-by: Igor Stoppa <igor.stoppa@huawei.com>
CC: Ashish Raj <araj@elisanet.fi>
CC: Carlos Chinea Perez <carlos.chinea.perez@huawei.com>
CC: Remi Denis Courmont <remi.denis.courmont@huawei.com>


---

diff --git a/include/asm-generic/vmlinux.lds.h b/include/asm-generic/vmlinux.lds.h
index e373e2e10f6a..4ca21ac3a1ec 100644
--- a/include/asm-generic/vmlinux.lds.h
+++ b/include/asm-generic/vmlinux.lds.h
@@ -303,6 +303,25 @@
 	. = __start_init_task + THREAD_SIZE;				\
 	__end_init_task = .;
 
+/*
+ * Allow architectures to handle rare_write_after_init data on their
+ * own by defining an empty RARE_WRITE_AFTER_INIT_DATA.
+ * However, it's important that pages containing RARE_WRITE data do not
+ * hold anything else, to avoid both accidentally unprotecting something
+ * that is supposed to stay read-only all the time and also to protect
+ * something else that is supposed to be writeable all the time.
+ */
+#ifndef RARE_WRITE_AFTER_INIT_DATA
+#define RARE_WRITE_AFTER_INIT_DATA(align)				\
+	. = ALIGN(PAGE_SIZE);						\
+	VMLINUX_SYMBOL(__start_rare_write_after_init) = .;		\
+	. = ALIGN(align);						\
+	*(.data..rare_write_after_init)					\
+	. = ALIGN(PAGE_SIZE);						\
+	VMLINUX_SYMBOL(__end_rare_write_after_init) = .;		\
+	. = ALIGN(align);
+#endif
+
 /*
  * Allow architectures to handle ro_after_init data on their
  * own by defining an empty RO_AFTER_INIT_DATA.
@@ -323,6 +342,7 @@
 		__start_rodata = .;					\
 		*(.rodata) *(.rodata.*)					\
 		RO_AFTER_INIT_DATA	/* Read only after init */	\
+		RARE_WRITE_AFTER_INIT_DATA(align) /* Rare write aft init */	\
 		KEEP(*(__vermagic))	/* Kernel version magic */	\
 		. = ALIGN(8);						\
 		__start___tracepoints_ptrs = .;				\
diff --git a/include/linux/cache.h b/include/linux/cache.h
index 750621e41d1c..af118f90551a 100644
--- a/include/linux/cache.h
+++ b/include/linux/cache.h
@@ -31,6 +31,23 @@
 #define __ro_after_init __attribute__((__section__(".data..ro_after_init")))
 #endif
 
+/*
+ * __rare_write_after_init is used to mark objects that after init cannot
+ * be modified directly (i.e. after mark_rodata_ro() has been called).
+ * These objects become effectively read-only, from the perspective of
+ * performing a direct write, like a variable assignment.
+ * However, they can be altered through a dedicated function.
+ * It is intended for those objects which are occasionally modified after
+ * init, however are they modified so seldomly that the extra cost from
+ * the indirect modification is either negligible or worth paying, for the
+ * sake of the hardening gained.
+ */
+#ifndef __rare_write_after_init
+#define __rare_write_after_init \
+		__attribute__((__section__(".data..rare_write_after_init")))
+#endif
+
+
 #ifndef ____cacheline_aligned
 #define ____cacheline_aligned __attribute__((__aligned__(SMP_CACHE_BYTES)))
 #endif
diff --git a/include/linux/rare_write.h b/include/linux/rare_write.h
new file mode 100644
index 000000000000..0ac9252a3f04
--- /dev/null
+++ b/include/linux/rare_write.h
@@ -0,0 +1,177 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * rare_write.h: Header for rare writes to statically allocated variables
+ *
+ * (C) Copyright 2018 Huawei Technologies Co. Ltd.
+ * Author: Igor Stoppa <igor.stoppa@huawei.com>
+ *
+ * Support for modifying "variables" residing in protected memory.
+ * They are write-protected against direct writes and can be altered only
+ * through special means.
+ */
+
+#ifndef _LINUX_RARE_WRITE_H
+#define _LINUX_RARE_WRITE_H
+
+#include <linux/mm.h>
+#include <linux/vmalloc.h>
+#include <linux/compiler.h>
+#include <linux/irqflags.h>
+
+extern long __start_rare_write_after_init;
+extern long __end_rare_write_after_init;
+
+enum rare_write_type {
+	RARE_WRITE_VIRT_ADDR,
+	RARE_WRITE_VMALLOC_ADDR,
+};
+
+static __always_inline
+bool rare_write_check_boundaries(const void *dst, size_t size)
+{
+	size_t start = (size_t)&__start_rare_write_after_init;
+	size_t end = (size_t)&__end_rare_write_after_init;
+	size_t low = (size_t)dst;
+	size_t high = (size_t)dst + size;
+
+	return likely(start <= low && low < high && high <= end);
+}
+
+/*
+ * This is the core of the rare write functionality.
+ * It doesn't perform any check on the validity of the target.
+ * The wrapper using it is supposed to apply sensible verification
+ * criteria, depending on the specific use-case and, to avoid unnecessary
+ * run-time checks, also specify the type of memory being modified.
+ */
+static __always_inline
+bool __raw_rare_write(const void *dst, const void *src,
+		      enum rare_write_type type, size_t n_bytes)
+{
+	size_t size;
+	unsigned long flags;
+
+	while (n_bytes) {
+		struct page *page;
+		void *base;
+		unsigned long offset;
+		size_t offset_complement;
+
+		local_irq_save(flags);
+		if (type == RARE_WRITE_VIRT_ADDR)
+			page = virt_to_page(dst);
+		else if (type == RARE_WRITE_VMALLOC_ADDR)
+			page = vmalloc_to_page(dst);
+		else
+			return false;
+		base = vmap(&page, 1, VM_MAP, PAGE_KERNEL);
+		if (WARN(!base, "failed to remap rare-write page"))
+			return false;
+		offset = (unsigned long)dst & ~PAGE_MASK;
+		offset_complement = ((size_t)PAGE_SIZE) - offset;
+		size = min(((int)n_bytes), ((int)offset_complement));
+		__write_once_size(base + offset, src, size);
+		vunmap(base);
+		dst += size;
+		src += size;
+		n_bytes -= size;
+		local_irq_restore(flags);
+	}
+	return true;
+}
+
+static __always_inline
+bool __rare_write(const void *dst, const void *src, size_t n_bytes)
+{
+
+	if (WARN(!rare_write_check_boundaries(dst, n_bytes),
+		 "Not a valid rare_write destination."))
+		return false;
+	return __raw_rare_write(dst, src, RARE_WRITE_VIRT_ADDR, n_bytes);
+
+}
+
+static __always_inline
+bool rare_write_char(const char *dst, const char val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_short(const short *dst, const short val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_ushort(const unsigned short *dst, const unsigned short val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_int(const int *dst, const int val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_uint(const unsigned int *dst, const unsigned int val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_long(const long *dst, const long val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_ulong(const unsigned long *dst, const unsigned long val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_longlong(const long long *dst, const long long val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_ulonglong(const unsigned long long *dst,
+			  const unsigned long long val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+static __always_inline
+bool rare_write_ptr(const void *dst, const void *val)
+{
+	return __rare_write(dst, &val, sizeof(val));
+}
+
+#define __rare_write_simple(dst_ptr, src_ptr)				\
+	__rare_write(dst_ptr, src_ptr, sizeof(*(src_ptr)))
+
+#define __rare_write_safe(dst_ptr, src_ptr,				\
+			  unique_dst_ptr, unique_src_ptr)		\
+({									\
+	typeof(dst_ptr) unique_dst_ptr = (dst_ptr);			\
+	typeof(src_ptr) unique_src_ptr = (src_ptr);			\
+									\
+	__rare_write(unique_dst_ptr, unique_src_ptr,			\
+		     sizeof(*(unique_src_ptr)));			\
+})
+
+#define rare_write(dst_ptr, src_ptr)					\
+	__builtin_choose_expr(__typecheck(dst_ptr, src_ptr),		\
+			      __rare_write_simple(dst_ptr, src_ptr),	\
+			      __rare_write_safe(dst_ptr, src_ptr,	\
+						__UNIQUE_ID(__dst_ptr),	\
+						__UNIQUE_ID(__src_ptr)))
+
+#define rare_write_array(dst_ptr, src_ptr, size)			\
+	__rare_write(dst_ptr, src_ptr, size)
+#endif
