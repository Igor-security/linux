Bottom: 5e375c1c4a1d03a8af4c2bd45b9c11b8f6c9a73b
Top:    12820e15f2f3b2a62299f02f98f1266ef3586f8d
Author: Igor Stoppa <igor.stoppa@huawei.com>
Date:   2018-05-08 20:18:01 +0400

prot_list


---

diff --git a/include/linux/pmalloc.h b/include/linux/pmalloc.h
index 0aab95074aa8..3887e571d1dc 100644
--- a/include/linux/pmalloc.h
+++ b/include/linux/pmalloc.h
@@ -40,16 +40,19 @@
 
 #define PMALLOC_REFILL_DEFAULT (0)
 #define PMALLOC_ALIGN_DEFAULT ARCH_KMALLOC_MINALIGN
-#define PMALLOC_RO 0
-#define PMALLOC_RW 1
+#define PMALLOC_RO		0x00
+#define PMALLOC_RW		0x01
+#define PMALLOC_START_RW	0x02
+#define PMALLOC_SHIFT_RW	0x04
+#define PMALLOC_SHIFT_RW_RO	0x08
 
 struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
-						bool rewritable,
-						unsigned short align_order);
+						unsigned short align_order,
+						bool mode);
 
 /**
  * pmalloc_create_pool() - create a protectable memory pool
- * @rewritable: can the data be altered after protection
+ * @mode: can the data be altered after protection
  *
  * Shorthand for pmalloc_create_custom_pool() with default argument:
  * * refill is set to PMALLOC_REFILL_DEFAULT
@@ -59,11 +62,11 @@ struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
  * * pointer to the new pool	- success
  * * NULL			- error
  */
-static inline struct pmalloc_pool *pmalloc_create_pool(bool rewritable)
+static inline struct pmalloc_pool *pmalloc_create_pool(bool mode)
 {
 	return pmalloc_create_custom_pool(PMALLOC_REFILL_DEFAULT,
-					  rewritable,
-					  PMALLOC_ALIGN_DEFAULT);
+					  PMALLOC_ALIGN_DEFAULT,
+					  mode);
 }
 
 
diff --git a/include/linux/prot_list.h b/include/linux/prot_list.h
new file mode 100644
index 000000000000..685c089d8911
--- /dev/null
+++ b/include/linux/prot_list.h
@@ -0,0 +1,27 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * prot_list.h: Header for Protectable Double Linked List
+ *
+ * (C) Copyright 2018 Huawei Technologies Co. Ltd.
+ * Author: Igor Stoppa <igor.stoppa@huawei.com>
+ */
+
+#ifndef _LINUX_PROT_LIST_H
+#define _LINUX_PROT_LIST_H
+
+#include <linux/pmalloc.h>
+#include <linux/list.h>
+#include <linux/kernel.h>
+
+struct prot_head {
+	struct prot_head *next, *prev;
+};
+
+static inline void INIT_PROT_LIST_HEAD(struct pmalloc_pool *pool,
+				       struct prot_head *list)
+{
+	struct prot_head head = {list, list};
+
+	pmalloc_rare_write(pool, list, &head, sizeof(struct prot_head));
+}
+#endif
diff --git a/mm/pmalloc.c b/mm/pmalloc.c
index ca7f10b50b25..7e2ba1b03c67 100644
--- a/mm/pmalloc.c
+++ b/mm/pmalloc.c
@@ -34,9 +34,9 @@ static DEFINE_MUTEX(pools_mutex);
  * @refill: the minimum size to allocate when in need of more memory.
  *          It will be rounded up to a multiple of PAGE_SIZE
  *          The value of 0 gives the default amount of PAGE_SIZE.
- * @rewritable: can the data be altered after protection
  * @align_order: log2 of the alignment to use when allocating memory
  *               Negative values give ARCH_KMALLOC_MINALIGN
+ * @mode: can the data be altered after protection
  *
  * Creates a new (empty) memory pool for allocation of protectable
  * memory. Memory will be allocated upon request (through pmalloc).
@@ -46,8 +46,8 @@ static DEFINE_MUTEX(pools_mutex);
  * * NULL			- error
  */
 struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
-						bool rewritable,
-						unsigned short align_order)
+						unsigned short align_order,
+						bool mode)
 {
 	struct pmalloc_pool *pool;
 
@@ -56,7 +56,7 @@ struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
 		return NULL;
 
 	pool->refill = refill ? PAGE_ALIGN(refill) : DEFAULT_REFILL_SIZE;
-	pool->rewritable = rewritable;
+	pool->mode = mode;
 	pool->align = 1UL << align_order;
 	mutex_init(&pool->mutex);
 
@@ -80,7 +80,7 @@ static int grow(struct pmalloc_pool *pool, size_t min_size)
 		return -ENOMEM;
 
 	area = find_vmap_area((unsigned long)addr);
-	tag_area(area, pool->rewritable);
+	tag_area(area, pool->mode);
 	pool->offset = get_area_pages_size(area);
 	llist_add(&area->area_list, &pool->vm_areas);
 	return 0;
@@ -193,7 +193,7 @@ bool pmalloc_rare_write(struct pmalloc_pool *pool, const void *destination,
 	 * that are not supposed to be modifiable.
 	 */
 	mutex_lock(&pool->mutex);
-	if (WARN(pool->rewritable != PMALLOC_RW,
+	if (WARN(pool->mode != PMALLOC_RW,
 		 "Attempting to modify non rewritable pool"))
 		goto out;
 	area = pool_get_area(pool, destination, n_bytes);
@@ -222,7 +222,7 @@ void pmalloc_make_pool_ro(struct pmalloc_pool *pool)
 	struct vmap_area *area;
 
 	mutex_lock(&pool->mutex);
-	pool->rewritable = false;
+	pool->mode = false;
 	llist_for_each_entry(area, pool->vm_areas.first, area_list)
 		protect_area(area);
 	mutex_unlock(&pool->mutex);
diff --git a/mm/pmalloc_helpers.h b/mm/pmalloc_helpers.h
index 538e37564f8f..8a0113d1bfba 100644
--- a/mm/pmalloc_helpers.h
+++ b/mm/pmalloc_helpers.h
@@ -26,7 +26,7 @@ struct pmalloc_pool {
 	size_t refill;
 	size_t offset;
 	size_t align;
-	bool rewritable;
+	bool mode;
 };
 
 #define VM_PMALLOC_PROTECTED_MASK (VM_PMALLOC | VM_PMALLOC_PROTECTED)
@@ -42,9 +42,9 @@ static __always_inline unsigned long area_flags(struct vmap_area *area)
 	return area->vm->flags & VM_PMALLOC_MASK;
 }
 
-static __always_inline void tag_area(struct vmap_area *area, bool rewritable)
+static __always_inline void tag_area(struct vmap_area *area, bool mode)
 {
-	if (rewritable == PMALLOC_RW)
+	if (mode == PMALLOC_RW)
 		area->vm->flags |= VM_PMALLOC_REWRITABLE_MASK;
 	else
 		area->vm->flags |= VM_PMALLOC;
diff --git a/security/integrity/ima/ima.h b/security/integrity/ima/ima.h
index 35fe91aa1fc9..f1e824b79728 100644
--- a/security/integrity/ima/ima.h
+++ b/security/integrity/ima/ima.h
@@ -105,7 +105,7 @@ struct ima_queue_entry {
 	struct list_head later;		/* place in ima_measurements list */
 	struct ima_template_entry *entry;
 };
-extern struct list_head ima_measurements;	/* list of all measurements */
+extern struct prot_head *ima_measurements_ptr;	/* list of all measurements */
 
 /* Some details preceding the binary serialized measurement list */
 struct ima_kexec_hdr {
diff --git a/security/integrity/ima/ima_fs.c b/security/integrity/ima/ima_fs.c
index fa540c0469da..52f6928e44c1 100644
--- a/security/integrity/ima/ima_fs.c
+++ b/security/integrity/ima/ima_fs.c
@@ -23,6 +23,7 @@
 #include <linux/rcupdate.h>
 #include <linux/parser.h>
 #include <linux/vmalloc.h>
+#include <linux/prot_list.h>
 
 #include "ima.h"
 
@@ -83,7 +84,7 @@ static void *ima_measurements_start(struct seq_file *m, loff_t *pos)
 
 	/* we need a lock since pos could point beyond last element */
 	rcu_read_lock();
-	list_for_each_entry_rcu(qe, &ima_measurements, later) {
+	list_for_each_entry_rcu(qe, (struct list_head *)ima_measurements_ptr, later) {
 		if (!l--) {
 			rcu_read_unlock();
 			return qe;
@@ -105,7 +106,7 @@ static void *ima_measurements_next(struct seq_file *m, void *v, loff_t *pos)
 	rcu_read_unlock();
 	(*pos)++;
 
-	return (&qe->later == &ima_measurements) ? NULL : qe;
+	return (&qe->later == (struct list_head *)ima_measurements_ptr) ? NULL : qe;
 }
 
 static void ima_measurements_stop(struct seq_file *m, void *v)
diff --git a/security/integrity/ima/ima_queue.c b/security/integrity/ima/ima_queue.c
index 418f35e38015..bf6f8aea54e5 100644
--- a/security/integrity/ima/ima_queue.c
+++ b/security/integrity/ima/ima_queue.c
@@ -24,11 +24,14 @@
 #include <linux/module.h>
 #include <linux/rculist.h>
 #include <linux/slab.h>
+#include <linux/prot_list.h>
 #include "ima.h"
 
 #define AUDIT_CAUSE_LEN_MAX 32
 
-LIST_HEAD(ima_measurements);	/* list of all measurements */
+static struct pmalloc_pool *ima_measurements_pool __ro_after_init;
+struct prot_head *ima_measurements_ptr __ro_after_init;
+
 #ifdef CONFIG_IMA_KEXEC
 static unsigned long binary_runtime_size;
 #else
@@ -107,7 +110,7 @@ static int ima_add_digest_entry(struct ima_template_entry *entry,
 	qe->entry = entry;
 
 	INIT_LIST_HEAD(&qe->later);
-	list_add_tail_rcu(&qe->later, &ima_measurements);
+	list_add_tail_rcu(&qe->later, (struct list_head *)ima_measurements_ptr);
 
 	atomic_long_inc(&ima_htable.len);
 	if (update_htable) {
@@ -212,3 +215,21 @@ int ima_restore_measurement_entry(struct ima_template_entry *entry)
 	mutex_unlock(&ima_extend_list_mutex);
 	return result;
 }
+
+static int __init init_ima_measurements(void)
+{
+	ima_measurements_pool = pmalloc_create_pool(PMALLOC_START_RW |
+						    PMALLOC_SHIFT_RW_RO);
+	ima_measurements_ptr = pmalloc(ima_measurements_pool,
+				       sizeof(struct list_head));
+	if (unlikely(!ima_measurements_ptr))
+		return -ENOMEM;
+	pr_info("next: %p   prev: %p",
+		ima_measurements_ptr->next, ima_measurements_ptr->prev);
+	INIT_PROT_LIST_HEAD(ima_measurements_pool,
+			    ima_measurements_ptr);
+	pr_info("next: %p   prev: %p",
+		ima_measurements_ptr->next, ima_measurements_ptr->prev);
+	return 0;
+}
+postcore_initcall(init_ima_measurements);
