Bottom: 4178026ae9e2a5aca3904955feaa69673165db5f
Top:    b814811b3f3d5fe9ebdcc8056c864025d42db7d6
Author: Igor Stoppa <igor.stoppa@huawei.com>
Date:   2018-04-18 13:43:09 +0400

proto_rare_write


---

diff --git a/include/linux/pmalloc.h b/include/linux/pmalloc.h
index 833f916b1ee8..7f463869e572 100644
--- a/include/linux/pmalloc.h
+++ b/include/linux/pmalloc.h
@@ -16,7 +16,7 @@
 /*
  * Library for dynamic allocation of pools of protectable memory.
  * A pool is a single linked list of vmap_area structures.
- * Whenever a pool is protected, all the areas it contain at that point
+ * Whenever a pool is protected, all the areas it contains at that point
  * are write protected.
  * More areas can be added and protected, in the same way.
  * Memory in a pool cannot be individually unprotected, but the pool can
@@ -24,29 +24,49 @@
  * Upon destruction of a certain pool, all the related memory is released,
  * including its metadata.
  *
+ * Depending on the type of protection that was choosen, the memory can be
+ * either completely read-only or it can support rare-writes.
+ *
+ * The rare-write mechanism is intended to provide no read overhead and
+ * still some form of protection, while a selected area is modified.
+ * This will incur into a penalty that is partially depending on the
+ * specific architecture, but in general is the price to pay for limiting
+ * the attack surface, while the change takes place.
+ *
+ * For additional safety, it is not possible to have in the same pool both
+ * rare-write and unmodifiable memory.
+ *
  * Pmalloc memory is intended to complement __read_only_after_init.
  * It can be used, for example, where there is a write-once variable, for
  * which it is not possible to know the initialization value before init
  * is completed (which is what __read_only_after_init requires).
  *
- * It can be useful also where the amount of data to protect is not known
- * at compile time and the memory can only be allocated dynamically.
+ * Rare write can also provide a compromise between freely rewritable
+ * memory and const.
+ *
+ * Another case where it can be useful is when the amount of data to
+ * protect is not known at compile time and the memory can only be
+ * allocated dynamically.
  *
  * Finally, it can be useful also when it is desirable to control
  * dynamically (for example throguh the command line) if something ought
  * to be protected or not, without having to rebuild the kernel (like in
- * the build used for a linux distro).
+ * the kernel binary files provided by a linux distro).
  */
 
 
 #define PMALLOC_REFILL_DEFAULT (0)
 #define PMALLOC_ALIGN_DEFAULT ARCH_KMALLOC_MINALIGN
+#define PMALLOC_RO 0
+#define PMALLOC_RW 1
 
 struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
+						bool rewritable,
 						unsigned short align_order);
 
 /**
  * pmalloc_create_pool() - create a protectable memory pool
+ * @rewritable: can the data be altered after protection
  *
  * Shorthand for pmalloc_create_custom_pool() with default argument:
  * * refill is set to PMALLOC_REFILL_DEFAULT
@@ -56,9 +76,10 @@ struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
  * * pointer to the new pool	- success
  * * NULL			- error
  */
-static inline struct pmalloc_pool *pmalloc_create_pool(void)
+static inline struct pmalloc_pool *pmalloc_create_pool(bool rewritable)
 {
 	return pmalloc_create_custom_pool(PMALLOC_REFILL_DEFAULT,
+					  rewritable,
 					  PMALLOC_ALIGN_DEFAULT);
 }
 
@@ -158,6 +179,7 @@ static inline char *pstrdup(struct pmalloc_pool *pool, const char *s)
 
 void pmalloc_protect_pool(struct pmalloc_pool *pool);
 
+void pmalloc_make_pool_ro(struct pmalloc_pool *pool);
 
 void pmalloc_destroy_pool(struct pmalloc_pool *pool);
 #endif
diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 69c12f21200f..d0b747a78271 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -21,7 +21,8 @@ struct notifier_block;		/* in notifier.h */
 #define VM_NO_GUARD		0x00000040      /* don't add guard page */
 #define VM_KASAN		0x00000080      /* has allocated kasan shadow memory */
 #define VM_PMALLOC		0x00000100	/* pmalloc area - see docs */
-#define VM_PMALLOC_PROTECTED	0x00000200	/* protected area - see docs */
+#define VM_PMALLOC_REWRITABLE	0x00000200	/* pmalloc rewritable area */
+#define VM_PMALLOC_PROTECTED	0x00000400	/* pmalloc protected area */
 /* bits [20..32] reserved for arch specific ioremap internals */
 
 /*
diff --git a/mm/pmalloc.c b/mm/pmalloc.c
index 1ca5145cb012..66b9d1092a2a 100644
--- a/mm/pmalloc.c
+++ b/mm/pmalloc.c
@@ -30,19 +30,23 @@ struct pmalloc_pool {
 	size_t refill;
 	size_t offset;
 	size_t align;
+	bool rewritable;
 };
 
 static LIST_HEAD(pools_list);
 static DEFINE_MUTEX(pools_mutex);
 
-static __always_inline void tag_area(struct vmap_area *area)
+static __always_inline void tag_area(struct vmap_area *area, bool rewritable)
 {
-	area->vm->flags |= VM_PMALLOC;
+	area->vm->flags |= VM_PMALLOC |
+			   (rewritable ? VM_PMALLOC_REWRITABLE : 0);
 }
 
 static __always_inline void untag_area(struct vmap_area *area)
 {
-	area->vm->flags &= ~(VM_PMALLOC | VM_PMALLOC_PROTECTED);
+	area->vm->flags &= ~(VM_PMALLOC |
+			     VM_PMALLOC_REWRITABLE |
+			     VM_PMALLOC_PROTECTED);
 }
 
 static __always_inline struct vmap_area *current_area(struct pmalloc_pool *pool)
@@ -64,6 +68,14 @@ static __always_inline void protect_area(struct vmap_area *area)
 	area->vm->flags |= VM_PMALLOC_PROTECTED;
 }
 
+
+static __always_inline void make_area_ro(struct vmap_area *area)
+{
+	area->vm->flags &= ~VM_PMALLOC_REWRITABLE;
+	protect_area(area);
+}
+
+
 static __always_inline void unprotect_area(struct vmap_area *area)
 {
 	if (likely(is_area_protected(area)))
@@ -109,6 +121,7 @@ static __always_inline bool space_needed(struct pmalloc_pool *pool, size_t size)
  * @refill: the minimum size to allocate when in need of more memory.
  *          It will be rounded up to a multiple of PAGE_SIZE
  *          The value of 0 gives the default amount of PAGE_SIZE.
+ * @rewritable: can the data be altered after protection
  * @align_order: log2 of the alignment to use when allocating memory
  *               Negative values give ARCH_KMALLOC_MINALIGN
  *
@@ -120,6 +133,7 @@ static __always_inline bool space_needed(struct pmalloc_pool *pool, size_t size)
  * * NULL			- error
  */
 struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
+						bool rewritable,
 						unsigned short align_order)
 {
 	struct pmalloc_pool *pool;
@@ -129,6 +143,7 @@ struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
 		return NULL;
 
 	pool->refill = refill ? PAGE_ALIGN(refill) : DEFAULT_REFILL_SIZE;
+	pool->rewritable = rewritable;
 	pool->align = 1UL << align_order;
 	mutex_init(&pool->mutex);
 
@@ -152,7 +167,7 @@ static int grow(struct pmalloc_pool *pool, size_t min_size)
 		return -ENOMEM;
 
 	area = find_vmap_area((unsigned long)addr);
-	tag_area(area);
+	tag_area(area, pool->rewritable);
 	pool->offset = area->vm->nr_pages * PAGE_SIZE;
 	llist_add(&area->area_list, &pool->vm_areas);
 	return 0;
@@ -219,6 +234,24 @@ void pmalloc_protect_pool(struct pmalloc_pool *pool)
 }
 EXPORT_SYMBOL(pmalloc_protect_pool);
 
+/**
+ * pmalloc_make_pool_ro() - drops rare-write permission from a pool
+ * @pool: the pool associated to the memory to make ro
+ *
+ * Drops the possibility to perform controlled writes from both the pool
+ * metadata and all the vm_area structures associated to the pool.
+ */
+void pmalloc_make_pool_ro(struct pmalloc_pool *pool)
+{
+	struct vmap_area *area;
+
+	mutex_lock(&pool->mutex);
+	pool->rewritable = false;
+	llist_for_each_entry(area, pool->vm_areas.first, area_list)
+		protect_area(area);
+	mutex_unlock(&pool->mutex);
+}
+EXPORT_SYMBOL(pmalloc_make_pool_ro);
 
 /**
  * pmalloc_destroy_pool() - destroys a pool and all the associated memory
@@ -247,3 +280,56 @@ void pmalloc_destroy_pool(struct pmalloc_pool *pool)
 	}
 }
 EXPORT_SYMBOL(pmalloc_destroy_pool);
+
+
+int pippo(void);
+
+int pippo(void)
+{
+	struct pmalloc_pool *pool;
+	char *var1, *var2;
+	struct page *page_from_array;
+	struct page *page_from_pointer;
+	struct vm_struct *vm_struct;
+	struct vmap_area *vmap_area;
+	void *phys;
+	void *remapped_addr;
+
+	pool = pmalloc_create_pool(PMALLOC_RW);
+	var1 = pmalloc(pool, PAGE_SIZE);
+	pr_info("pippo var1              = 0x%p", var1);
+
+	vmap_area = find_vmap_area((unsigned long)var1);
+	pr_info("pippo vmap_area         = 0x%p", vmap_area);
+
+	vm_struct = vmap_area->vm;
+	pr_info("pippo vm_struct         = 0x%p", vm_struct);
+
+	page_from_array = vm_struct->pages[0];
+	pr_info("pippo page_from_array   = 0x%p", page_from_array);
+
+	page_from_pointer = vmalloc_to_page(var1);
+	pr_info("pippo page_from_pointer = 0x%p", page_from_pointer);
+
+	phys = (void *)page_to_phys(page_from_pointer);
+	pr_info("pippo phys              = 0x%p", phys);
+
+	*var1 = 25;
+	pmalloc_protect_pool(pool);
+
+	remapped_addr = vmap(&page_from_array, 1, VM_MAP, PAGE_KERNEL);
+	pr_info("pippo remapped_addr = 0x%p", remapped_addr);
+
+	var2 = (char*)remapped_addr;
+	pr_info("pippo var2 = %d", (int)*var2);
+
+	*var2 = 19;
+	pr_info("pippo var2 = %d", (int)*var2);
+	pr_info("pippo var1 = %d", (int)*var1);
+	vunmap(remapped_addr);
+//	*var2 = 1;
+	pr_info("pippo var1 = %d", (int)*var1);
+//	*var1 = 1;
+	return 0;
+}
+core_initcall(pippo);
