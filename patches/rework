Bottom: 1b10cdfe9dc551880be79880f250fd16574a4b2c
Top:    294c0e35c644a01d9b9a77fc9d7a1e8ce4c07f60
Author: Igor Stoppa <igor.stoppa@huawei.com>
Date:   2018-05-10 20:49:39 +0400

rework


---

diff --git a/include/linux/cache.h b/include/linux/cache.h
index 750621e41d1c..af118f90551a 100644
--- a/include/linux/cache.h
+++ b/include/linux/cache.h
@@ -31,6 +31,23 @@
 #define __ro_after_init __attribute__((__section__(".data..ro_after_init")))
 #endif
 
+/*
+ * __rare_write_after_init is used to mark objects that after init cannot
+ * be modified directly (i.e. after mark_rodata_ro() has been called).
+ * These objects become effectively read-only, from the perspective of
+ * performing a direct write, like a variable assignment.
+ * However, they can be altered through a dedicated function.
+ * It is intended for those objects which are occasionally modified after
+ * init, however are they modified so seldomly that the extra cost from
+ * the indirect modification is either negligible or worth paying, for the
+ * sake of the hardening gained.
+ */
+#ifndef __rare_write_after_init
+#define __rare_write_after_init \
+		__attribute__((__section__(".data..rare_write_after_init")))
+#endif
+
+
 #ifndef ____cacheline_aligned
 #define ____cacheline_aligned __attribute__((__aligned__(SMP_CACHE_BYTES)))
 #endif
diff --git a/include/linux/pmalloc.h b/include/linux/pmalloc.h
index 0aab95074aa8..b8e3d133c59f 100644
--- a/include/linux/pmalloc.h
+++ b/include/linux/pmalloc.h
@@ -9,9 +9,28 @@
 #ifndef _LINUX_PMALLOC_H
 #define _LINUX_PMALLOC_H
 
+#ifndef CONFIG_PROTECTABLE_MEMORY
 
-#include <linux/string.h>
-#include <linux/slab.h>
+/**
+ * check_pmalloc_object - hardened usercopy stub if pmalloc is unavailable
+ * @ptr: the beginning of the memory to check
+ * @n: the size of the memory to check
+ * @to_user: copy to userspace or from userspace
+ *
+ * If pmalloc is disabled, there is nothing to check.
+ */
+static inline
+void check_pmalloc_object(const void *ptr, unsigned long n, bool to_user)
+{
+}
+
+#else
+
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
 /*
  * Library for dynamic allocation of pools of protectable memory.
@@ -37,19 +56,250 @@
  * rare-write and unmodifiable memory.
  */
 
+#include <linux/set_memory.h>
+#include <linux/mm.h>
+#include <linux/vmalloc.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+#include <linux/mutex.h>
+#include <linux/list.h>
+#include <linux/llist.h>
 
 #define PMALLOC_REFILL_DEFAULT (0)
 #define PMALLOC_ALIGN_DEFAULT ARCH_KMALLOC_MINALIGN
-#define PMALLOC_RO 0
-#define PMALLOC_RW 1
+
+
+/*
+ * A pool can be set either for rare-write or read-only mode.
+ * In both cases, it can be managed either manually or automatically.
+ */
+#define PMALLOC_RO		0x00
+#define PMALLOC_RW		0x01
+#define PMALLOC_AUTO		0x02
+#define PMALLOC_MANUAL_RO	PMALLOC_RO
+#define PMALLOC_MANUAL_RW	PMALLOC_RW
+#define PMALLOC_AUTO_RO		(PMALLOC_RO | PMALLOC_AUTO)
+#define PMALLOC_AUTO_RW		(PMALLOC_RW | PMALLOC_AUTO)
+
+#define VM_PMALLOC_PROTECTED_MASK (VM_PMALLOC | VM_PMALLOC_PROTECTED)
+#define VM_PMALLOC_REWRITABLE_MASK \
+	(VM_PMALLOC | VM_PMALLOC_REWRITABLE)
+#define VM_PMALLOC_PROTECTED_REWRITABLE_MASK \
+	(VM_PMALLOC | VM_PMALLOC_REWRITABLE | VM_PMALLOC_PROTECTED)
+#define VM_PMALLOC_MASK \
+	(VM_PMALLOC | VM_PMALLOC_REWRITABLE | VM_PMALLOC_PROTECTED)
+
+
+struct pmalloc_pool {
+	struct mutex mutex;
+	struct list_head pool_node;
+	struct llist_head vm_areas;
+	size_t refill;
+	size_t offset;
+	size_t align;
+	uint8_t mode;
+};
+
+
+/*
+ * Helper functions, not part of the API.
+ * They are implemented as inlined functions, instead of macros, for
+ * additional type-checking, however they are not meant to be called
+ * directly by pmalloc users.
+ */
+static __always_inline unsigned long __area_flags(struct vmap_area *area)
+{
+	return area->vm->flags & VM_PMALLOC_MASK;
+}
+
+static __always_inline void __tag_area(struct vmap_area *area, uint32_t mask)
+{
+	area->vm->flags |= mask;
+}
+
+static __always_inline void __untag_area(struct vmap_area *area)
+{
+	area->vm->flags &= ~VM_PMALLOC_MASK;
+}
+
+static __always_inline struct vmap_area *__current_area(struct pmalloc_pool
+							*pool)
+{
+	return llist_entry(pool->vm_areas.first, struct vmap_area,
+			   area_list);
+}
+
+static __always_inline
+bool __area_matches_mask(struct vmap_area *area, unsigned long mask)
+{
+	return (area->vm->flags & mask) == mask;
+}
+
+static __always_inline bool __is_area_protected(struct vmap_area *area)
+{
+	return __area_matches_mask(area, VM_PMALLOC_PROTECTED_MASK);
+}
+
+static __always_inline bool __is_area_rewritable(struct vmap_area *area)
+{
+	return __area_matches_mask(area, VM_PMALLOC_REWRITABLE_MASK);
+}
+
+static __always_inline void __protect_area(struct vmap_area *area)
+{
+	if (unlikely(__is_area_protected(area)))
+		return;
+	set_memory_ro(area->va_start, area->vm->nr_pages);
+	area->vm->flags |= VM_PMALLOC_PROTECTED_MASK;
+}
+
+static __always_inline void __make_area_ro(struct vmap_area *area)
+{
+	area->vm->flags &= ~VM_PMALLOC_REWRITABLE;
+	__protect_area(area);
+}
+
+static __always_inline void __unprotect_area(struct vmap_area *area)
+{
+	if (likely(__is_area_protected(area)))
+		set_memory_rw(area->va_start, area->vm->nr_pages);
+	__untag_area(area);
+}
+
+static __always_inline void __destroy_area(struct vmap_area *area)
+{
+	WARN(!__is_area_protected(area), "Destroying unprotected area.");
+	__unprotect_area(area);
+	vfree((void *)area->va_start);
+}
+
+static __always_inline bool __empty(struct pmalloc_pool *pool)
+{
+	return unlikely(llist_empty(&pool->vm_areas));
+}
+
+static __always_inline bool __protected(struct pmalloc_pool *pool)
+{
+	return __is_area_protected(__current_area(pool));
+}
+
+static inline bool __exhausted(struct pmalloc_pool *pool, size_t size)
+{
+	size_t space_before;
+	size_t space_after;
+
+	space_before = round_down(pool->offset, pool->align);
+	space_after = pool->offset - space_before;
+	return unlikely(space_after < size && space_before < size);
+}
+
+static __always_inline
+bool __space_needed(struct pmalloc_pool *pool, size_t size)
+{
+	return __empty(pool) || __protected(pool) || __exhausted(pool, size);
+}
+
+static __always_inline size_t __get_area_pages_size(struct vmap_area *area)
+{
+	return area->vm->nr_pages * PAGE_SIZE;
+}
+
+static inline int is_pmalloc_object(const void *ptr, const unsigned long n)
+{
+	struct vm_struct *area;
+	unsigned long start = (unsigned long)ptr;
+	unsigned long end = start + n;
+	unsigned long area_end;
+
+	if (likely(!is_vmalloc_addr(ptr)))
+		return false;
+
+	area = vmalloc_to_page(ptr)->area;
+	if (unlikely(!(area->flags & VM_PMALLOC)))
+		return false;
+
+	area_end = area->nr_pages * PAGE_SIZE + (unsigned long)area->addr;
+	return (start <= end) && (end <= area_end);
+}
+
+static __always_inline size_t __get_area_pages_end(struct vmap_area *area)
+{
+	return area->va_start + __get_area_pages_size(area);
+}
+
+static __always_inline
+bool __area_contains_range(struct vmap_area *area, const void *addr,
+			   size_t n_bytes)
+{
+	size_t area_end = __get_area_pages_end(area);
+	size_t range_start = (size_t)addr;
+	size_t range_end = range_start + n_bytes;
+
+	return (area->va_start <= range_start) &&
+	       (range_start < area_end) &&
+	       (area->va_start <= range_end) &&
+	       (range_end <= area_end);
+}
+
+static __always_inline
+struct vmap_area *__pool_get_area(struct pmalloc_pool *pool,
+				  const void *addr, size_t n_bytes)
+{
+	struct vmap_area *area;
+
+	llist_for_each_entry(area, pool->vm_areas.first, area_list)
+		if (__area_contains_range(area, addr,  n_bytes))
+			return area;
+	return NULL;
+}
+
+/*
+ * Pmalloc API
+  */
+void __noreturn usercopy_abort(const char *name, const char *detail,
+			       bool to_user, unsigned long offset,
+			       unsigned long len);
+
+/**
+ * check_pmalloc_object - helper for hardened usercopy
+ * @ptr: the beginning of the memory to check
+ * @n: the size of the memory to check
+ * @to_user: copy to userspace or from userspace
+ *
+ * If the check is ok, it will fall-through, otherwise it will abort.
+ * The function is inlined, to minimize the performance impact of the
+ * extra check to perform on a typically hot path.
+ * Micro benchmarking with QEMU shows a reduction of the time spent in this
+ * fragment by 60%, when inlined.
+ */
+static inline
+void check_pmalloc_object(const void *ptr, unsigned long n, bool to_user)
+{
+	int retv;
+
+	retv = is_pmalloc_object(ptr, n);
+	if (unlikely(retv)) {
+		if (unlikely(!to_user))
+			usercopy_abort("pmalloc",
+				       "writing to pmalloc object", to_user,
+				       (const unsigned long)ptr, n);
+		if (retv < 0)
+			usercopy_abort("pmalloc",
+				       "invalid pmalloc object", to_user,
+				       (const unsigned long)ptr, n);
+	}
+}
+
+void pmalloc_init_custom_pool(struct pmalloc_pool *pool, size_t refill,
+			      unsigned short align_order, uint8_t mode);
 
 struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
-						bool rewritable,
-						unsigned short align_order);
+						unsigned short align_order,
+						uint8_t mode);
 
 /**
  * pmalloc_create_pool() - create a protectable memory pool
- * @rewritable: can the data be altered after protection
+ * @mode: can the data be altered after protection
  *
  * Shorthand for pmalloc_create_custom_pool() with default argument:
  * * refill is set to PMALLOC_REFILL_DEFAULT
@@ -59,17 +309,15 @@ struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
  * * pointer to the new pool	- success
  * * NULL			- error
  */
-static inline struct pmalloc_pool *pmalloc_create_pool(bool rewritable)
+static inline struct pmalloc_pool *pmalloc_create_pool(uint8_t mode)
 {
 	return pmalloc_create_custom_pool(PMALLOC_REFILL_DEFAULT,
-					  rewritable,
-					  PMALLOC_ALIGN_DEFAULT);
+					  PMALLOC_ALIGN_DEFAULT,
+					  mode);
 }
 
-
 void *pmalloc(struct pmalloc_pool *pool, size_t size);
 
-
 /**
  * pzalloc() - zero-initialized version of pmalloc()
  * @pool: handle to the pool to be used for memory allocation
@@ -91,7 +339,6 @@ static inline void *pzalloc(struct pmalloc_pool *pool, size_t size)
 	return ptr;
 }
 
-
 /**
  * pmalloc_array() - array version of pmalloc()
  * @pool: handle to the pool to be used for memory allocation
@@ -105,15 +352,14 @@ static inline void *pzalloc(struct pmalloc_pool *pool, size_t size)
  * * NULL		- error
  */
 
-static inline void *pmalloc_array(struct pmalloc_pool *pool, size_t n,
-				  size_t size)
+static inline
+void *pmalloc_array(struct pmalloc_pool *pool, size_t n, size_t size)
 {
 	if (unlikely(size != 0) && unlikely(n > SIZE_MAX / size))
 		return NULL;
 	return pmalloc(pool, n * size);
 }
 
-
 /**
  * pcalloc() - array version of pzalloc()
  * @pool: handle to the pool to be used for memory allocation
@@ -126,15 +372,14 @@ static inline void *pmalloc_array(struct pmalloc_pool *pool, size_t n,
  * * the pmalloc result	- success
  * * NULL		- error
  */
-static inline void *pcalloc(struct pmalloc_pool *pool, size_t n,
-			    size_t size)
+static inline
+void *pcalloc(struct pmalloc_pool *pool, size_t n, size_t size)
 {
 	if (unlikely(size != 0) && unlikely(n > SIZE_MAX / size))
 		return NULL;
 	return pzalloc(pool, n * size);
 }
 
-
 /**
  * pstrdup() - duplicate a string, using pmalloc()
  * @pool: handle to the pool to be used for memory allocation
@@ -159,8 +404,85 @@ static inline char *pstrdup(struct pmalloc_pool *pool, const char *s)
 	return buf;
 }
 
+
+/*
+ * Non-API, lock-unsafe rare-write, meant only for internal use, where the
+ * locking must span over multiple rare-write operations to the same pool,
+ * for ensuring data coherency.
+ */
+static __always_inline
+bool __pmalloc_rare_write(struct pmalloc_pool *pool, const void *dst,
+			  const void *src, size_t n_bytes)
+{
+	struct vmap_area *area;
+	struct page *page;
+	void *base;
+	size_t size;
+	unsigned long offset;
+
+	/*
+	 * The following sanitation is meant to make life harder for
+	 * attempts at using ROP/JOP to call this function against areas
+	 * that are not supposed to be modifiable.
+	 */
+	area = __pool_get_area(pool, dst, n_bytes);
+	if (WARN(!area, "Destination range not in pool"))
+		return false;
+	if (WARN(!__is_area_rewritable(area),
+		 "Attempting to modify non rewritable area"))
+		return false;
+	while (n_bytes) {
+		size_t offset_complement;
+		page = vmalloc_to_page(dst);
+		base = vmap(&page, 1, VM_MAP, PAGE_KERNEL);
+		if (WARN(!base, "failed to remap rewritable page"))
+			return false;
+		offset = (unsigned long)dst & ~PAGE_MASK;
+		offset_complement = ((size_t)PAGE_SIZE) - offset;
+		size = min(((int)n_bytes), ((int)offset_complement));
+		memcpy(base + offset, src, size);
+		vunmap(base);
+		dst += size;
+		src += size;
+		n_bytes -= size;
+	}
+	return true;
+}
+
+/**
+ * pmalloc_rare_write() - alters the content of a rewritable pool
+ * @pool: the pool associated to the memory to write-protect
+ * @destination: where to write the new data
+ * @source: the location of the data to replicate into the pool
+ * @n_bytes: the size of the region to modify
+ *
+ * The rare-write functionality is fully implemented as __always_inline,
+ * to prevent having an internal function call that is capable of modifying
+ * write protected memory.
+ * Fully inlining the function allows the compiler to optimize away its
+ * interface, making it harder for an attacker to hijack it.
+ * This still leaves the door open to attacks that might try to reuse part
+ * of the code, by jumping in the middle of the function, however it can
+ * be mitigated by having a compiler plugin that enforces Control Flow
+ * Integrity (CFI).
+ * Any addition/modification to the rare-write path must follow the same
+ * approach.
+
+ * Return:
+ * * true	- success
+ * * false	- error
+ */
+static __always_inline
 bool pmalloc_rare_write(struct pmalloc_pool *pool, const void *destination,
-			const void *source, size_t n_bytes);
+			const void *source, size_t n_bytes)
+{
+	bool retval;
+
+	mutex_lock(&pool->mutex);
+	retval = __pmalloc_rare_write(pool, destination, source, n_bytes);
+	mutex_unlock(&pool->mutex);
+	return retval;
+}
 
 void pmalloc_protect_pool(struct pmalloc_pool *pool);
 
@@ -168,3 +490,4 @@ void pmalloc_make_pool_ro(struct pmalloc_pool *pool);
 
 void pmalloc_destroy_pool(struct pmalloc_pool *pool);
 #endif
+#endif
diff --git a/include/linux/prot_list.h b/include/linux/prot_list.h
new file mode 100644
index 000000000000..78bf18603131
--- /dev/null
+++ b/include/linux/prot_list.h
@@ -0,0 +1,94 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * prot_list.h: Header for Protectable Double Linked List
+ *
+ * (C) Copyright 2018 Huawei Technologies Co. Ltd.
+ * Author: Igor Stoppa <igor.stoppa@huawei.com>
+ */
+
+#ifndef _LINUX_PROT_LIST_H
+#define _LINUX_PROT_LIST_H
+
+#include <linux/pmalloc.h>
+#include <linux/list.h>
+#include <linux/kernel.h>
+
+struct prot_list_pool {
+	struct pmalloc_pool pool;
+};
+
+struct prot_head {
+	struct prot_head *next, *prev;
+};
+
+struct prot_list_pool *prot_list_create_custom_pool(size_t refill,
+						    unsigned short align_order);
+
+static inline
+struct prot_list_pool *prot_list_create_pool(void)
+{
+	return prot_list_create_custom_pool(PMALLOC_REFILL_DEFAULT,
+					    PMALLOC_ALIGN_DEFAULT);
+}
+
+static inline void INIT_PROT_LIST_HEAD(struct prot_list_pool *pool,
+				       struct prot_head *list)
+{
+	struct prot_head head = {list, list};
+
+	pmalloc_rare_write(&pool->pool, list, &head, sizeof(struct prot_head));
+}
+
+static inline struct prot_head *PROT_LIST_HEAD(struct prot_list_pool *pool)
+{
+	struct prot_head *head;
+
+	head = pmalloc(&pool->pool, sizeof(struct prot_head));
+	if (WARN(!head, "Could not allocate protected list head."))
+		return NULL;
+	INIT_PROT_LIST_HEAD(pool, head);
+	return head;
+
+}
+
+#define prot_list_append(pool, head, src, node) \
+	__prot_list_add(pool, head, src, sizeof(*src), \
+			((uintptr_t)&(src)->node) - (uintptr_t)(src))
+
+#define prot_list_prepend(pool, head, src, node) \
+	__prot_list_add(pool, (head)->prev, src, sizeof(*(src)), \
+			((uintptr_t)&(src)->node) - (uintptr_t)(src))
+
+static inline bool __prot_list_add(struct prot_list_pool *pool,
+				   struct prot_head *head,
+				   void *src, size_t src_size,
+				   uintptr_t offset)
+{
+	void *dst;
+	bool retval;
+	struct prot_head *src_list;
+	void *p;
+
+	dst = pmalloc(&pool->pool, src_size);
+	if (WARN(!head, "Could not allocate protected list head."))
+		return false;
+	mutex_lock(&pool->pool.mutex);
+	src_list = src + offset;
+	src_list->prev = head;
+	src_list->next = head->next;
+	retval = __pmalloc_rare_write(&pool->pool, dst, src, src_size);
+	if (WARN(!retval, "Failed to init list element."))
+		goto out;
+	p = (void *)(offset + (uintptr_t)dst);
+	retval = __pmalloc_rare_write(&pool->pool, &head->next->prev, &p,
+				      sizeof(p));
+	if (WARN(!retval, "Failed to hook to next element."))
+		goto out;
+	retval = __pmalloc_rare_write(&pool->pool, &head->next, &p, sizeof(p));
+	if (WARN(!retval, "Failed to hook to previous element."))
+		goto out;
+out:
+	mutex_unlock(&pool->pool.mutex);
+	return retval;
+}
+#endif
diff --git a/include/linux/rare_write.h b/include/linux/rare_write.h
new file mode 100644
index 000000000000..95176e36af44
--- /dev/null
+++ b/include/linux/rare_write.h
@@ -0,0 +1,100 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * rare_write.h: Header for rare writes to statically allocated variables
+ *
+ * (C) Copyright 2018 Huawei Technologies Co. Ltd.
+ * Author: Igor Stoppa <igor.stoppa@huawei.com>
+ *
+ * Support for modifying "variables" residing in protected memory.
+ * They are write-protected against direct writes and can be altered only
+ * through special means.
+ */
+
+#ifndef _LINUX_RARE_WRITE_H
+#define _LINUX_RARE_WRITE_H
+
+extern long __start_rare_write_after_init;
+extern long __end_rare_write_after_init;
+
+enum rare_write_type {
+	RARE_WRITE_VIRT_ADDR,
+	RARE_WRITE_VMALLOC_ADDR,
+};
+
+__always_inline bool __rare_write_check_bounds(void *dst)
+{
+	return (dst >= (void *)&__start_rare_write_after_init) &&
+	       (dst < (void *)&__end_rare_write_after_init);
+}
+
+/*
+ * This is the core of the rare write functionality.
+ * It doesn't perform any check on the validity of the target.
+ * The wrapper using it is supposed to apply sensible verification
+ * criteria, depending on the specific use-case and, for avoiding run-time
+ * checks, also specify the type of memory being modified.
+ */
+__always_inline
+bool __raw_rare_write(void *dst, void *src, enum rare_write_type type,
+		      size_t n_bytes)
+{
+	size_t size;
+
+	while (n_bytes) {
+		struct page *page;
+		void *base;
+		unsigned long offset;
+		size_t offset_complement;
+
+		if (type == RARE_WRITE_VIRT_ADDR)
+			page = virt_to_page(dst);
+		else
+			page = vmalloc_to_page(dst);
+		base = vmap(&page, 1, VM_MAP, PAGE_KERNEL);
+		if (WARN(!base, "failed to remap rare-write page"))
+			return false;
+		offset = (unsigned long)dst & ~PAGE_MASK;
+		offset_complement = ((size_t)PAGE_SIZE) - offset;
+		size = min(((int)n_bytes), ((int)offset_complement));
+		memcpy(base + offset, src, size);
+		vunmap(base);
+		dst += size;
+		src += size;
+		n_bytes -= size;
+	}
+	return true;
+}
+
+__always_inline bool __rare_write(void *dst, void *src, size_t n_bytes)
+{
+
+	if (WARN(!(__rare_write_check_bounds(dst)),
+		 "Not a valid rare_write destination."))
+		return false;
+	return __raw_rare_write(dst, src, RARE_WRITE_VIRT_ADDR, n_bytes);
+
+}
+
+#define __rare_write_simple(dst_ptr, src_ptr)				\
+	__rare_write(dst_ptr, src_ptr, sizeof(*(src_ptr)))
+
+#define __rare_write_safe(dst_ptr, src_ptr,				\
+			  unique_dst_ptr, unique_src_ptr)		\
+({									\
+	typeof(dst_ptr) unique_dst_ptr = (dst_ptr);			\
+	typeof(src_ptr) unique_src_ptr = (src_ptr);			\
+									\
+	__rare_write(unique_dst_ptr, unique_src_ptr,			\
+		     sizeof(*(unique_src_ptr)));			\
+})
+
+#define rare_write(dst_ptr, src_ptr)					\
+	__builtin_choose_expr(__typecheck(dst_ptr, src_ptr),		\
+			      __rare_write_simple(dst_ptr, src_ptr),	\
+			      __rare_write_safe(dst_ptr, src_ptr,	\
+						__UNIQUE_ID(__dst_ptr),	\
+						__UNIQUE_ID(__src_ptr)))
+
+#define rare_write_array(dst_ptr, src_ptr, size)			\
+	__rare_write(dst_ptr, src_ptr, size)
+#endif
diff --git a/init/main.c b/init/main.c
index fd37315835b4..d866b605c353 100644
--- a/init/main.c
+++ b/init/main.c
@@ -1053,6 +1053,7 @@ static inline void mark_readonly(void)
 }
 #endif
 
+int test_static_rare_write(void);
 static int __ref kernel_init(void *unused)
 {
 	int ret;
@@ -1064,6 +1065,7 @@ static int __ref kernel_init(void *unused)
 	jump_label_invalidate_initmem();
 	free_initmem();
 	mark_readonly();
+	test_static_rare_write();
 	system_state = SYSTEM_RUNNING;
 	numa_default_policy();
 
diff --git a/lib/Makefile b/lib/Makefile
index ce20696d5a92..d949af6c1b2a 100644
--- a/lib/Makefile
+++ b/lib/Makefile
@@ -265,3 +265,4 @@ obj-$(CONFIG_GENERIC_LSHRDI3) += lshrdi3.o
 obj-$(CONFIG_GENERIC_MULDI3) += muldi3.o
 obj-$(CONFIG_GENERIC_CMPDI2) += cmpdi2.o
 obj-$(CONFIG_GENERIC_UCMPDI2) += ucmpdi2.o
+obj-$(CONFIG_PROTECTABLE_MEMORY) += prot_list.o
diff --git a/lib/prot_list.c b/lib/prot_list.c
new file mode 100644
index 000000000000..20eacbac6cad
--- /dev/null
+++ b/lib/prot_list.c
@@ -0,0 +1,23 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * prot_list.c: protected double linked list
+ *
+ * (C) Copyright 2018 Huawei Technologies Co. Ltd.
+ * Author: Igor Stoppa <igor.stoppa@huawei.com>
+ */
+
+#include <linux/prot_list.h>
+
+struct prot_list_pool *prot_list_create_custom_pool(size_t refill,
+						    unsigned short align_order)
+{
+	struct prot_list_pool *pool;
+
+	pool = kzalloc(sizeof(struct prot_list_pool), GFP_KERNEL);
+	if (WARN(!pool, "Could not allocate pool meta data."))
+		return NULL;
+	pmalloc_init_custom_pool(&pool->pool, refill, align_order,
+				 PMALLOC_AUTO_RW);
+	return pool;
+}
+EXPORT_SYMBOL(prot_list_create_custom_pool);
diff --git a/mm/pmalloc.c b/mm/pmalloc.c
index ca7f10b50b25..d74aea870fe4 100644
--- a/mm/pmalloc.c
+++ b/mm/pmalloc.c
@@ -21,7 +21,6 @@
 #include <asm/page.h>
 
 #include <linux/pmalloc.h>
-#include "pmalloc_helpers.h"
 
 static LIST_HEAD(pools_list);
 static DEFINE_MUTEX(pools_mutex);
@@ -29,14 +28,40 @@ static DEFINE_MUTEX(pools_mutex);
 #define MAX_ALIGN_ORDER (ilog2(sizeof(void *)))
 #define DEFAULT_REFILL_SIZE PAGE_SIZE
 
+/**
+ * pmalloc_init_custom_pool() - initialize a protectable memory pool
+ * @pool: the pointer to the struct pmalloc_pool to initialize
+ * @refill: the minimum size to allocate when in need of more memory.
+ *          It will be rounded up to a multiple of PAGE_SIZE
+ *          The value of 0 gives the default amount of PAGE_SIZE.
+ * @align_order: log2 of the alignment to use when allocating memory
+ *               Negative values give ARCH_KMALLOC_MINALIGN
+ * @mode: can the data be altered after protection
+ *
+ * Initializes an empty memory pool, for allocation of protectable
+ * memory. Memory will be allocated upon request (through pmalloc).
+ */
+void pmalloc_init_custom_pool(struct pmalloc_pool *pool, size_t refill,
+			      unsigned short align_order, uint8_t mode)
+{
+	pool->refill = refill ? PAGE_ALIGN(refill) : DEFAULT_REFILL_SIZE;
+	pool->mode = mode;
+	pool->align = 1UL << align_order;
+	mutex_init(&pool->mutex);
+	mutex_lock(&pools_mutex);
+	list_add(&pool->pool_node, &pools_list);
+	mutex_unlock(&pools_mutex);
+}
+EXPORT_SYMBOL(pmalloc_init_custom_pool);
+
 /**
  * pmalloc_create_custom_pool() - create a new protectable memory pool
  * @refill: the minimum size to allocate when in need of more memory.
  *          It will be rounded up to a multiple of PAGE_SIZE
  *          The value of 0 gives the default amount of PAGE_SIZE.
- * @rewritable: can the data be altered after protection
  * @align_order: log2 of the alignment to use when allocating memory
  *               Negative values give ARCH_KMALLOC_MINALIGN
+ * @mode: can the data be altered after protection
  *
  * Creates a new (empty) memory pool for allocation of protectable
  * memory. Memory will be allocated upon request (through pmalloc).
@@ -46,53 +71,50 @@ static DEFINE_MUTEX(pools_mutex);
  * * NULL			- error
  */
 struct pmalloc_pool *pmalloc_create_custom_pool(size_t refill,
-						bool rewritable,
-						unsigned short align_order)
+						unsigned short align_order,
+						uint8_t mode)
 {
 	struct pmalloc_pool *pool;
 
 	pool = kzalloc(sizeof(struct pmalloc_pool), GFP_KERNEL);
 	if (WARN(!pool, "Could not allocate pool meta data."))
 		return NULL;
-
-	pool->refill = refill ? PAGE_ALIGN(refill) : DEFAULT_REFILL_SIZE;
-	pool->rewritable = rewritable;
-	pool->align = 1UL << align_order;
-	mutex_init(&pool->mutex);
-
-	mutex_lock(&pools_mutex);
-	list_add(&pool->pool_node, &pools_list);
-	mutex_unlock(&pools_mutex);
+	pmalloc_init_custom_pool(pool, refill, align_order, mode);
 	return pool;
 }
 EXPORT_SYMBOL(pmalloc_create_custom_pool);
 
-
 static int grow(struct pmalloc_pool *pool, size_t min_size)
 {
 	void *addr;
-	struct vmap_area *area;
+	struct vmap_area *new_area;
 	unsigned long size;
+	uint32_t tag_mask;
 
 	size = (min_size > pool->refill) ? min_size : pool->refill;
 	addr = vmalloc(size);
 	if (WARN(!addr, "Failed to allocate %zd bytes", PAGE_ALIGN(size)))
 		return -ENOMEM;
 
-	area = find_vmap_area((unsigned long)addr);
-	tag_area(area, pool->rewritable);
-	pool->offset = get_area_pages_size(area);
-	llist_add(&area->area_list, &pool->vm_areas);
+	new_area = find_vmap_area((unsigned long)addr);
+	tag_mask = VM_PMALLOC;
+	if (pool->mode & PMALLOC_RW)
+		tag_mask |= VM_PMALLOC_REWRITABLE;
+	__tag_area(new_area, tag_mask);
+	if (pool->mode == PMALLOC_AUTO_RW)
+		__protect_area(new_area);
+	if (pool->mode == PMALLOC_AUTO_RO) {
+		struct vmap_area *old_area;
+
+		old_area = container_of(pool->vm_areas.first,
+					struct vmap_area, area_list);
+		__protect_area(old_area);
+	}
+	pool->offset = __get_area_pages_size(new_area);
+	llist_add(&new_area->area_list, &pool->vm_areas);
 	return 0;
 }
 
-static void *reserve_mem(struct pmalloc_pool *pool, size_t size)
-{
-	pool->offset = round_down(pool->offset - size, pool->align);
-	return (void *)(current_area(pool)->va_start + pool->offset);
-
-}
-
 /**
  * pmalloc() - allocate protectable memory from a pool
  * @pool: handle to the pool to be used for memory allocation
@@ -114,10 +136,11 @@ void *pmalloc(struct pmalloc_pool *pool, size_t size)
 	void *retval = NULL;
 
 	mutex_lock(&pool->mutex);
-	if (unlikely(space_needed(pool, size)) &&
+	if (unlikely(__space_needed(pool, size)) &&
 	    unlikely(grow(pool, size)))
 			goto out;
-	retval = reserve_mem(pool, size);
+	pool->offset = round_down(pool->offset - size, pool->align);
+	retval = (void *)(__current_area(pool)->va_start + pool->offset);
 out:
 	mutex_unlock(&pool->mutex);
 	return retval;
@@ -142,73 +165,11 @@ void pmalloc_protect_pool(struct pmalloc_pool *pool)
 
 	mutex_lock(&pool->mutex);
 	llist_for_each_entry(area, pool->vm_areas.first, area_list)
-		protect_area(area);
+		__protect_area(area);
 	mutex_unlock(&pool->mutex);
 }
 EXPORT_SYMBOL(pmalloc_protect_pool);
 
-static inline bool rare_write(const void *destination,
-			      const void *source, size_t n_bytes)
-{
-	struct page *page;
-	void *base;
-	size_t size;
-	unsigned long offset;
-
-	while (n_bytes) {
-		page = vmalloc_to_page(destination);
-		base = vmap(&page, 1, VM_MAP, PAGE_KERNEL);
-		if (WARN(!base, "failed to remap rewritable page"))
-			return false;
-		offset = (unsigned long)destination & ~PAGE_MASK;
-		size = min(n_bytes, (size_t)PAGE_SIZE - offset);
-		memcpy(base + offset, source, size);
-		vunmap(base);
-		destination += size;
-		source += size;
-		n_bytes -= size;
-	}
-}
-
-/**
- * pmalloc_rare_write() - alters the content of a rewritable pool
- * @pool: the pool associated to the memory to write-protect
- * @destination: where to write the new data
- * @source: the location of the data to replicate into the pool
- * @n_bytes: the size of the region to modify
- *
- * Return:
- * * true	- success
- * * false	- error
- */
-bool pmalloc_rare_write(struct pmalloc_pool *pool, const void *destination,
-			const void *source, size_t n_bytes)
-{
-	bool retval = false;
-	struct vmap_area *area;
-
-	/*
-	 * The following sanitation is meant to make life harder for
-	 * attempts at using ROP/JOP to call this function against pools
-	 * that are not supposed to be modifiable.
-	 */
-	mutex_lock(&pool->mutex);
-	if (WARN(pool->rewritable != PMALLOC_RW,
-		 "Attempting to modify non rewritable pool"))
-		goto out;
-	area = pool_get_area(pool, destination, n_bytes);
-	if (WARN(!area, "Destination range not in pool"))
-		goto out;
-	if (WARN(!is_area_rewritable(area),
-		 "Attempting to modify non rewritable area"))
-		goto out;
-	rare_write(destination, source, n_bytes);
-	retval = true;
-out:
-	mutex_unlock(&pool->mutex);
-	return retval;
-}
-EXPORT_SYMBOL(pmalloc_rare_write);
 
 /**
  * pmalloc_make_pool_ro() - drops rare-write permission from a pool
@@ -222,9 +183,9 @@ void pmalloc_make_pool_ro(struct pmalloc_pool *pool)
 	struct vmap_area *area;
 
 	mutex_lock(&pool->mutex);
-	pool->rewritable = false;
+	pool->mode = false;
 	llist_for_each_entry(area, pool->vm_areas.first, area_list)
-		protect_area(area);
+		__protect_area(area);
 	mutex_unlock(&pool->mutex);
 }
 EXPORT_SYMBOL(pmalloc_make_pool_ro);
@@ -252,7 +213,7 @@ void pmalloc_destroy_pool(struct pmalloc_pool *pool)
 		tmp = cursor;
 		cursor = cursor->next;
 		area = llist_entry(tmp, struct vmap_area, area_list);
-		destroy_area(area);
+		__destroy_area(area);
 	}
 }
 EXPORT_SYMBOL(pmalloc_destroy_pool);
diff --git a/mm/pmalloc_helpers.h b/mm/pmalloc_helpers.h
deleted file mode 100644
index 538e37564f8f..000000000000
--- a/mm/pmalloc_helpers.h
+++ /dev/null
@@ -1,210 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/*
- * pmalloc_helpers.h: Protectable Memory Allocator internal header
- *
- * (C) Copyright 2018 Huawei Technologies Co. Ltd.
- * Author: Igor Stoppa <igor.stoppa@huawei.com>
- */
-
-#ifndef _MM_VMALLOC_HELPERS_H
-#define _MM_VMALLOC_HELPERS_H
-
-#ifndef CONFIG_PROTECTABLE_MEMORY
-
-static inline void check_pmalloc_object(const void *ptr, unsigned long n,
-					bool to_user)
-{
-}
-
-#else
-
-#include <linux/set_memory.h>
-struct pmalloc_pool {
-	struct mutex mutex;
-	struct list_head pool_node;
-	struct llist_head vm_areas;
-	size_t refill;
-	size_t offset;
-	size_t align;
-	bool rewritable;
-};
-
-#define VM_PMALLOC_PROTECTED_MASK (VM_PMALLOC | VM_PMALLOC_PROTECTED)
-#define VM_PMALLOC_REWRITABLE_MASK \
-	(VM_PMALLOC | VM_PMALLOC_REWRITABLE)
-#define VM_PMALLOC_PROTECTED_REWRITABLE_MASK \
-	(VM_PMALLOC | VM_PMALLOC_REWRITABLE | VM_PMALLOC_PROTECTED)
-#define VM_PMALLOC_MASK \
-	(VM_PMALLOC | VM_PMALLOC_REWRITABLE | VM_PMALLOC_PROTECTED)
-
-static __always_inline unsigned long area_flags(struct vmap_area *area)
-{
-	return area->vm->flags & VM_PMALLOC_MASK;
-}
-
-static __always_inline void tag_area(struct vmap_area *area, bool rewritable)
-{
-	if (rewritable == PMALLOC_RW)
-		area->vm->flags |= VM_PMALLOC_REWRITABLE_MASK;
-	else
-		area->vm->flags |= VM_PMALLOC;
-}
-
-static __always_inline void untag_area(struct vmap_area *area)
-{
-	area->vm->flags &= ~VM_PMALLOC_MASK;
-}
-
-static __always_inline struct vmap_area *current_area(struct pmalloc_pool *pool)
-{
-	return llist_entry(pool->vm_areas.first, struct vmap_area,
-			   area_list);
-}
-
-static __always_inline bool area_matches_mask(struct vmap_area *area,
-					      unsigned long mask)
-{
-	return (area->vm->flags & mask) == mask;
-}
-
-static __always_inline bool is_area_protected(struct vmap_area *area)
-{
-	return area_matches_mask(area, VM_PMALLOC_PROTECTED_MASK);
-}
-
-static __always_inline bool is_area_rewritable(struct vmap_area *area)
-{
-	return area_matches_mask(area, VM_PMALLOC_REWRITABLE_MASK);
-}
-
-static __always_inline void protect_area(struct vmap_area *area)
-{
-	if (unlikely(is_area_protected(area)))
-		return;
-	set_memory_ro(area->va_start, area->vm->nr_pages);
-	area->vm->flags |= VM_PMALLOC_PROTECTED_MASK;
-}
-
-static __always_inline void make_area_ro(struct vmap_area *area)
-{
-	area->vm->flags &= ~VM_PMALLOC_REWRITABLE;
-	protect_area(area);
-}
-
-static __always_inline void unprotect_area(struct vmap_area *area)
-{
-	if (likely(is_area_protected(area)))
-		set_memory_rw(area->va_start, area->vm->nr_pages);
-	untag_area(area);
-}
-
-static __always_inline void destroy_area(struct vmap_area *area)
-{
-	WARN(!is_area_protected(area), "Destroying unprotected area.");
-	unprotect_area(area);
-	vfree((void *)area->va_start);
-}
-
-static __always_inline bool empty(struct pmalloc_pool *pool)
-{
-	return unlikely(llist_empty(&pool->vm_areas));
-}
-
-static __always_inline bool protected(struct pmalloc_pool *pool)
-{
-	return is_area_protected(current_area(pool));
-}
-
-static inline bool exhausted(struct pmalloc_pool *pool, size_t size)
-{
-	size_t space_before;
-	size_t space_after;
-
-	space_before = round_down(pool->offset, pool->align);
-	space_after = pool->offset - space_before;
-	return unlikely(space_after < size && space_before < size);
-}
-
-static __always_inline bool space_needed(struct pmalloc_pool *pool, size_t size)
-{
-	return empty(pool) || protected(pool) || exhausted(pool, size);
-}
-
-static __always_inline size_t get_area_pages_size(struct vmap_area *area)
-{
-	return area->vm->nr_pages * PAGE_SIZE;
-}
-
-static inline int is_pmalloc_object(const void *ptr, const unsigned long n)
-{
-	struct vm_struct *area;
-	unsigned long start = (unsigned long)ptr;
-	unsigned long end = start + n;
-	unsigned long area_end;
-
-	if (likely(!is_vmalloc_addr(ptr)))
-		return false;
-
-	area = vmalloc_to_page(ptr)->area;
-	if (unlikely(!(area->flags & VM_PMALLOC)))
-		return false;
-
-	area_end = area->nr_pages * PAGE_SIZE + (unsigned long)area->addr;
-	return (start <= end) && (end <= area_end);
-}
-
-void __noreturn usercopy_abort(const char *name, const char *detail,
-			       bool to_user, unsigned long offset,
-			       unsigned long len);
-
-static inline void check_pmalloc_object(const void *ptr, unsigned long n,
-					bool to_user)
-{
-	int retv;
-
-	retv = is_pmalloc_object(ptr, n);
-	if (unlikely(retv)) {
-		if (unlikely(!to_user))
-			usercopy_abort("pmalloc",
-				       "trying to write to pmalloc object",
-				       to_user, (const unsigned long)ptr, n);
-		if (retv < 0)
-			usercopy_abort("pmalloc",
-				       "invalid pmalloc object",
-				       to_user, (const unsigned long)ptr, n);
-	}
-}
-
-static __always_inline size_t get_area_pages_end(struct vmap_area *area)
-{
-	return area->va_start + get_area_pages_size(area);
-}
-
-static __always_inline bool area_contains_range(struct vmap_area *area,
-						const void *addr,
-						size_t n_bytes)
-{
-	size_t area_end = get_area_pages_end(area);
-	size_t range_start = (size_t)addr;
-	size_t range_end = range_start + n_bytes;
-
-	return (area->va_start <= range_start) &&
-	       (range_start < area_end) &&
-	       (area->va_start <= range_end) &&
-	       (range_end <= area_end);
-}
-
-static __always_inline
-struct vmap_area *pool_get_area(struct pmalloc_pool *pool,
-				const void *addr, size_t n_bytes)
-{
-	struct vmap_area *area;
-
-	llist_for_each_entry(area, pool->vm_areas.first, area_list)
-		if (area_contains_range(area, addr,  n_bytes))
-			return area;
-	return NULL;
-}
-
-#endif
-#endif
diff --git a/mm/test_pmalloc.c b/mm/test_pmalloc.c
index 43b764e2fb02..ae69c92d011b 100644
--- a/mm/test_pmalloc.c
+++ b/mm/test_pmalloc.c
@@ -8,11 +8,10 @@
 
 #include <linux/init.h>
 #include <linux/module.h>
-#include <linux/pmalloc.h>
 #include <linux/mm.h>
 #include <linux/bug.h>
-
-#include "pmalloc_helpers.h"
+#include <linux/pmalloc.h>
+#include <linux/prot_list.h>
 
 #define SIZE_1 (PAGE_SIZE * 3)
 #define SIZE_2 1000
@@ -179,6 +178,80 @@ static int test_rare_write(void)
 	return 0;
 }
 
+#include <linux/sched.h>
+#include <linux/thread_info.h>
+
+static int victim __rare_write_after_init = 23;
+#include <linux/rare_write.h>
+
+
+int test_static_rare_write(void)
+{
+	int src = 11;
+
+	pr_notice("QQQQQQ Victim is %d", victim);
+	rare_write(&victim, &src);
+	pr_info("QQQQQQ start: 0x%016lx", (unsigned long)&__start_rare_write_after_init);
+	pr_info("QQQQQQ victim: 0x%016lx", (unsigned long)&victim);
+	pr_info("QQQQQQ end: 0x%016lx", (unsigned long)&__end_rare_write_after_init);
+	pr_notice("QQQQQQ Victim is %d", victim);
+}
+EXPORT_SYMBOL(test_static_rare_write);
+
+struct test_data {
+	int data_int;
+	struct prot_head list;
+	unsigned long long data_ulong;
+};
+
+static int test_prot_list(void)
+{
+	struct prot_list_pool *pool;
+	struct prot_head *head;
+	struct prot_head *cursor;
+	struct test_data data;
+	int i;
+
+	/* Create a pool for the protectable list. */
+	pool = prot_list_create_pool();
+	if (WARN(!pool, "could not create pool"))
+		return -ENOMEM;
+
+	head = PROT_LIST_HEAD(pool);
+	for (i = 0; i < 100; i++) {
+		data.data_int = i;
+		data.data_ulong = i * i;
+		if (i % 2)
+			prot_list_append(pool, head, &data, list);
+		else
+			prot_list_prepend(pool, head, &data, list);
+	}
+	for (cursor = head->next; cursor != head; cursor = cursor->next) {
+		struct test_data *data;
+
+		data = container_of(cursor, struct test_data, list);
+
+		pr_info("cursor: 0x%08lx  data_int: %02d ",
+			(unsigned long)cursor, data->data_int);
+	}
+/*	{
+		struct test_data *data;
+
+		data = container_of(cursor->next, struct test_data, list);
+		data->data_int += 5;
+	}*/
+	for (cursor = head->prev; cursor != head; cursor = cursor->prev) {
+		struct test_data *data;
+
+		data = container_of(cursor, struct test_data, list);
+
+		pr_info("cursor: 0x%08lx  data_int: %02d ",
+			(unsigned long)cursor, data->data_int);
+	}
+
+	return 0;
+}
+
 /**
  * test_pmalloc()  -main entry point for running the test cases
  */
@@ -192,6 +265,8 @@ static int __init test_pmalloc_init_module(void)
 		       test_is_pmalloc_object())))
 		return -1;
 	test_rare_write();
+	test_prot_list();
+//	test_static_rare_write();
 	return 0;
 }
 
diff --git a/mm/usercopy.c b/mm/usercopy.c
index 6c47bd765033..c4aa7ced36b7 100644
--- a/mm/usercopy.c
+++ b/mm/usercopy.c
@@ -26,7 +26,6 @@
 #include <linux/sched/clock.h>
 #include <asm/sections.h>
 
-#include "pmalloc_helpers.h"
 
 /*
  * Checks if a given pointer and length is contained by the current
